{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OgGOQ3MpULl",
        "colab_type": "text"
      },
      "source": [
        "Создаем 4-слойную нейронную сеть в датасете MNIST. Входной слой состоит из 28*28 = 784 пикселей с оттенками серого, которые составляют входные данные в датасете MNIST. Входные данные далее проходят через два скрытых слоя, каждый из которых содержит 200 узлов, использующих линейную выпрямительную функцию активации (ReLU). Выходной слой с десятью узлами, соответствующими десяти рукописным цифрам от 0 до 9. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm_rjVQ6mSye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwIjKfTyqJUQ",
        "colab_type": "text"
      },
      "source": [
        "Чтобы создать нейронную сеть в PyTorch, используется класс nn.Module. Чтобы им воспользоваться, необходимо наследование, что позволит использовать весь функционал базового класса nn.Module, но при этом еще имеется возможность переписать базовый класс для конструирования модели или прямого прохождения через сеть.    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JbjY51xpLT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "258ee2ff-8f49-48af-db4d-6fd14ab0d7fd"
      },
      "source": [
        "def create_nn(batch_size=250, learning_rate=0.1,\n",
        "              log_interval=119):\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),\n",
        "        batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            # создание 3 полносвязных слоев\n",
        "            self.fc1 = nn.Linear(28 * 28, 200)\n",
        "            self.fc2 = nn.Linear(200, 200)\n",
        "            self.fc3 = nn.Linear(200, 10)\n",
        "\n",
        "        # метод прохода данных по сети    \n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return F.log_softmax(x)\n",
        "\n",
        "    net = Net()\n",
        "    print(\"Structura Seti: \",net)\n",
        "\n",
        "    # оптимизатор на основе стохастического градиентного спуска, скорость \n",
        "    # обучения = 0.1\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    # функция потерь - отрицательное логарифмическое правдоподобие\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # цикл тренировки\n",
        "    for epoch in range(5):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            \n",
        "            data, target = Variable(data), Variable(target)\n",
        "            # изменим размер (1,28,28)  до 1-мерного случая для 28 х 28 = 784 входных\n",
        "            # узла.\n",
        "            # (batch_size, 784)\n",
        "            \n",
        "            data = data.view(-1, 28*28)\n",
        "            # явный перезапуск градиентов\n",
        "            optimizer.zero_grad()\n",
        "            # подаём часть данных. net_out - логарифмич. выходной softmax\n",
        "            net_out = net(data)\n",
        "            # функция потери отрицательного логарифмического правдоподобия\n",
        "            loss = criterion(net_out, target)\n",
        "            # обратное распространение ошибки\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.5f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                           100. * batch_idx / len(train_loader), loss.data))\n",
        "       \n",
        "    # тестирование\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        data = data.view(-1, 28 * 28)\n",
        "        net_out = net(data)\n",
        "        # суммируем  batch потери\n",
        "        test_loss += criterion(net_out, target).data\n",
        "        pred = net_out.data.max(1)[1]  # индекс масимальной лог вероятности\n",
        "        correct += pred.eq(target.data).sum() # счетчик правильных ответов сети\n",
        "\n",
        "    # Суммируя выходы функции .eq(), получаем счетчик количества раз, когда нейронная сеть выдает правильный ответ. \n",
        "    # По накопленной сумме правильных предсказаний можно определить общую точность сети на тренировочном датасете.\n",
        "    # Наконец, проходя по каждой партии входных данных, выводим среднее значение функции потери и точность модели:\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_opt = 2\n",
        "    if run_opt == 1:\n",
        "        simple_gradient()\n",
        "        \n",
        "    elif run_opt == 2:\n",
        "        create_nn()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Structura Seti:  Net(\n",
            "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
            ")\n",
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.31425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [29750/60000 (50%)]\tLoss: 0.09859\n",
            "Train Epoch: 0 [59500/60000 (99%)]\tLoss: 0.18816\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.05509\n",
            "Train Epoch: 1 [29750/60000 (50%)]\tLoss: 0.10004\n",
            "Train Epoch: 1 [59500/60000 (99%)]\tLoss: 0.08605\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.03997\n",
            "Train Epoch: 2 [29750/60000 (50%)]\tLoss: 0.09247\n",
            "Train Epoch: 2 [59500/60000 (99%)]\tLoss: 0.09819\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.01412\n",
            "Train Epoch: 3 [29750/60000 (50%)]\tLoss: 0.03850\n",
            "Train Epoch: 3 [59500/60000 (99%)]\tLoss: 0.10513\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.02020\n",
            "Train Epoch: 4 [29750/60000 (50%)]\tLoss: 0.00896\n",
            "Train Epoch: 4 [59500/60000 (99%)]\tLoss: 0.06427\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "nTest set: Average loss: 0.0003, Accuracy: 9796/10000 (98.0%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZpgPIZlrkJD",
        "colab_type": "text"
      },
      "source": [
        "На выходе получаем прогресс на протяжении эпох тренировки и ошибку нейросети в этот момент.   \n",
        "После тренировки сети получаем следующие результаты:  \n",
        "Средняя потеря: 0,0003, точность: 9796/10000 (98,0%).\n"
      ]
    }
  ]
}